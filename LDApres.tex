%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Introduction to Bayesian Statistics]{How to Become a Bayesian?} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{Ying Zhang} % Your name
\institute[Renmin University of China] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
Renmin University of China \\ % Your institution for the title page
\medskip
\textit{zyjannis@ruc.edu.cn} % Your email address
}
\date{\today} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Overview} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}

%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------
\beamerdefaultoverlayspecification{<+->}
%------------------------------------------------

\section{Introduction to Bayesian Framework} % Sections can be created in order to organize your presentation into discrete blocks, all sections and subsections are automatically printed in the table of contents as an overview of the talk
%------------------------------------------------
\begin{frame}
\sectionpage % Print the title page as the first slide
\end{frame}

\subsection{Priori, Likelihood and Posteriori}
\begin{frame}
\frametitle{Priori, Likelihood and Posteriori}
\begin{itemize}
\item Bayes Formula $$p(\theta\mid y) = \frac{p(\theta)p(y\mid\theta)}{p(y)}$$
\item Prior $\theta$ $\sim$ Prior Distribution $p(\theta)$ \\
Determined from past information or subjective assessment.
\item Observations $y\mid \theta$ $\sim$ Likelihood $p(y \mid \theta)$ \\
Given the parameter $\theta$, the observed data $y$'s distribution. 
\item Posterior $\theta \mid y$ $\sim$ Posterior Distribution $p(\theta \mid y)$ \\
Updatad distribution of $\theta$ based on its prior and observed data.
\item $$p(\theta \mid y) = \frac{p(\theta)p(y\mid \theta)}{\int p(\theta)p(y\mid \theta) d\theta} \propto p(\theta)p(y\mid \theta)$$ \\
\end{itemize}
\end{frame}

\subsection{Types of Prior}
\begin{frame}
\frametitle{What is prior?}
What exactly is prior when we talk about it?
\begin{itemize}
\item Past experience
\item Historical Research
\item Subjective Beliefs
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Type of Priors}
We can define three types of priors according to the information they contain
\begin{itemize}
\item Informative Priors \\
Prior distributions giving numerical information that is crucial to estimation of the model.
\item Non-informative Priors\\
Uniform or nearly so, and basically allow the information from the likelihood to be interpreted probabilistically.
\item Weakly Informative Priors\\
Not supplying any controversial information but are strong enough to pull the data away from inappropriate inferences that are consistent with the likelihood.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Common Types of Priors}
What kinds of prior do we usally use? 
\begin{itemize}
\item Experts' Prior \\
Prior distributions obtained via consulting experts.
\item Conjugate Priors\\
The prior distribution and the posterior distribution are from the same distribution family.\\
For example, if $\theta \sim $beta distribution then $\theta\mid y \sim beta$ distribution
\item Non-informative Priors\\
Uniform Prior or Jeffrey Prior
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Conjugate Prior}
\begin{itemize}
\item The prior distribution and the posterior distribution are from the same distribution family.
\item Example : $$\theta \sim \text{Beta}(\alpha,\beta) \quad p(\theta) \propto \theta^{\alpha -1}(1 - \theta)^{\beta - 1} \quad \theta \in (0,1) $$
$$y\mid \theta \sim \text{Binomial}(n,\theta) \quad \quad p(y\mid \theta) \propto \theta^{y}(1 - \theta)^{n-y}$$
\item Hence we can derive the posterior $$p(\theta\mid y) \propto p(y\mid \theta)\cdot p(\theta) \propto \theta^{\alpha + y -1}(1 - \theta)^{\beta +n - y - 1}$$
Therefore, $$\theta \mid y \sim \text{Beta}(\alpha + y,\beta +n - y) \quad \theta \in (0,1) $$ 
\item $\theta$'s prior and posterior are both Beta distribution.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Why we use Conjugate Prior?} 
\begin{itemize}
\item {\bf They simplify the computation!}\\
We can easily derive the posterior distribution if we use conjugate prior.
\item Common Conjugate Families
\begin{figure}
\includegraphics[width=0.75\linewidth]{fig0.png}\\
\footnotesize{\centering{Table of Common Conjugate families}}
\end{figure}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Non-informative Prior}
\begin{itemize}
\item Uniform $$p(\theta) \propto 1$$
\item Example 1: $$p(\theta) = \frac{1}{2} \quad \quad \theta \in (0,2) $$
\item Example 2: $$p(\theta) \propto 1 \quad \quad \theta \in (-\infty,\infty) $$
Is that correct?
\item Prior is not a distribution! Its density cannot be integrated to 1.\\
We call this prior is {\bf improper}.
\item Improper prior can {\bf sometimes} lead to proper posterior.
\item As long as it can lead to proper posterior, the prior can be useful.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Non-informative Prior}
\begin{itemize}
\item Example 3: $$p(\theta) \propto 1 \quad \quad \theta \in (-\infty,\infty) $$
$$y\mid \theta \sim N(\theta, 1)$$
\item Hence we can derive the posterior $$p(\theta\mid y) \propto p(y\mid \theta)\cdot 1 = \frac{1}{\sqrt{2\pi}}e^{-\frac{(\theta - y)^2}{2}}$$
Therefore, $$\theta \mid y \sim N(y,1) \quad \theta \in (-\infty,\infty) $$ 
\item It's a proper posterior!
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Jeffrey Prior}
\begin{itemize}
\item Do we have any other choice for non-informative prior?\\
\item Yes! That is Jeffrey Prior.
\item $$p(\theta) \propto [J(\theta)]^{\frac{1}{2}}$$ where $J(\theta)$ is the {\em Fisher Information} for $\theta$
$$J(\theta) = E((\frac{d logp(y\mid \theta)}{d\theta})^2\mid \theta)= - E(\frac{d^{2} logp(y\mid \theta)}{d\theta^{2}}\mid \theta)$$
\item Jeffrey's Invariance Principal:\\
No matter how I parametrize $\theta$, the prior density $p(\theta)$ is equivalent.
\item  $$p(\theta) \propto [J(\theta)]^{\frac{1}{2}} \quad \text{Let} \quad \phi = h(\theta) \quad \text{One-to-One mapping}$$ We can prove that $$p(\phi) \propto [J(\phi)]^{\frac{1}{2}}$$ 
\end{itemize}
\end{frame}


%------------------------------------------------
\section{Bayesian Hierarchical Model} % Sections can be created in order to organize your presentation into discrete blocks, all sections and subsections are automatically printed in the table of contents as an overview of the talk
%------------------------------------------------

\begin{frame}
\sectionpage % Print the title page as the first slide
\end{frame}

\subsection{How to set the Hyperparameters?} % A subsection can be created just before a set of slides with a common theme to further break down your presentation into chunks
\begin{frame}
\frametitle{Toy Example}
\begin{figure}
\includegraphics[width=1.05\linewidth]{fig1.png}\\
\footnotesize{\centering{Table by \cite{p3}}}
\end{figure}
\begin{itemize}
\item The table displays the values of $\frac{y_{i}}{n_{i}}$ : $i = 1,2,3,...,70$ \\
 \centering{(number of rats with tumor) / (total number of rats)}
\item Tumor Incidence of rats in historical control groups and current group of rats, from Tarone (1982). 
\end{itemize}
\end{frame}

%--------------------------------------------------

\begin{frame}
\frametitle{Model Initialization}
\begin{itemize}
\item Suppose $\theta$ is the probability that the rat had tumor.
\item Suppose $$y \mid \theta \sim \text{Binomial}(n,\theta)$$
\item $$\theta \sim \text{Beta}(\alpha, \beta)$$
\item Since Beta-Binomial is conjugate, so we can derive the posterior of $\theta$ easily
\item $$\theta\mid y \sim \text{Beta}(\alpha + y, \beta + n - y)$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Model Initialization}
\begin{figure}
\includegraphics[width=0.7\linewidth]{fig8.png}\\
\footnotesize{\centering{Figure by \cite{p3}}}
\end{figure}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Toy Example}
How to set the $\alpha$ and $\beta$?\\ 
(We call the parameters in prior distribution {\bf \em hyperparameter})
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{How to set the priors?}

\begin{block}{Fixed Prior Distribution(Informative Prior)}
\begin{itemize}
\item {\bf We knew} that $\theta \sim$ Beta Distribution with known mean and variance. 
\item $\theta$ vary due to differences in rats and experimental conditions. 
\item Find the corresponding $\alpha$, $\beta$. 
\item $\theta \sim$ Beta$(\alpha,\beta)$ as its prior distribution.
\end{itemize}
\end{block}

\begin{block}{Approximate estimate using Historical Data}
\begin{itemize}
\item Use Historical Data's Mean and Variance to estimate $\alpha$ and $\beta$. 
\item $$y_i \mid \theta \sim Binomial(n_i, \theta)$$
$$\theta \sim \text{Beta}(\hat{\alpha},\hat{\beta})$$
\item $\theta\mid y_1, y_2,\ldots,y_{71} \sim \text{Beta}(\hat{\alpha} + \sum_{i = 1}^{71} y_i ,\hat{\beta} + \sum_{i = 1}^{71}n_{i} -  \sum_{i = 1}^{71} y_i)$
\end{itemize}
\end{block}
\end{frame}


%------------------------------------------------

\begin{frame}
\frametitle{Approximate estimate using Historical Data}
\begin{itemize}
\item Bayes Estimate $$\begin{aligned}&E\{\theta \mid y\}\\ 
&= \frac{\hat{\alpha} +  \sum_{i = 1}^{71} y_i}{\hat{\alpha} +\hat{\beta} + \sum_{i = 1}^{71}n_{i}} \\
&= \frac{\hat{\alpha}}{\hat{\alpha} +\hat{\beta} + \sum_{i = 1}^{71}n_{i}} +  \frac{ \sum_{i = 1}^{71} y_i}{\hat{\alpha} +\hat{\beta} + \sum_{i = 1}^{71}n_{i}}\\
&= (\frac{\hat{\alpha}}{\hat{\alpha} +\hat{\beta}})(\frac{\hat{\alpha} + \hat{\beta}}{\hat{\alpha} +\hat{\beta} + \sum_{i = 1}^{71}n_{i}}) + (\frac{\sum_{i = 1}^{71} y_i}{\sum_{i = 1}^{71}n_{i}})(\frac{\sum_{i = 1}^{71}n_{i}}{\hat{\alpha} +\hat{\beta} + \sum_{i = 1}^{71}n_{i}})\\
&=  (\frac{\sum_{i = 1}^{70} y_i}{\sum_{i = 1}^{70}n_{i}})(\frac{\hat{\alpha} + \hat{\beta}}{\hat{\alpha} +\hat{\beta} + \sum_{i = 1}^{71}n_{i}}) + (\frac{\sum_{i = 1}^{71} y_i}{\sum_{i = 1}^{71}n_{i}})(\frac{\sum_{i = 1}^{71}n_{i}}{\hat{\alpha} +\hat{\beta} + \sum_{i = 1}^{71}n_{i}})
\end{aligned}$$
\item Is that Correct?
\item \centering{\bf NO!}
\item Overestimate the precision of the posterior. (Data Used Twice)
\end{itemize}
\end{frame}

%------------------------------------------------------
\begin{frame}
\frametitle{Set the Hyperparameters without Data}
\begin{block}{Do we have to use data to set the hyperparameters?}
\begin{itemize}
\item In most cases in reality, we are not sure what how to set the priors scientifically.
\item However, the hyperparameters of the prior may not be that important.
\item If lacking information, use non-informative prior such as $Uniform(0,1) = Beta(1,1)$
\item In this case,
$$y_i \mid \theta_i \sim \text{Binomial}(n_i,\theta_i) $$
$$\theta_i \sim \text{Uniform}(0,1)$$ for $i = 1,2,...,70,71$
\end{itemize}
\end{block}
\end{frame}

%------------------------------------------------------
\subsection{Can we regard hyperparameters in prior as random variables?}
\begin{frame}
\frametitle{Set one more level of Hierarchical Model}
\begin{block}{Regard $\alpha$ \& $\beta$ as Random Variables}
\begin{itemize}
\item If we want to model the uncertainty of $\alpha$ and $\beta$,
\item We can assign a prior distributions for $\alpha$ and $\beta$ respectively.
\item Just add one more level of Hierarchical Model.
\item For example,
$$y_i \mid \theta_i \sim \text{Binomial}(n_i,\theta_i) $$
$$\theta_i \mid \alpha, \beta \sim \text{Beta}(\alpha,\beta)$$
$$\alpha \sim Gamma(1,2) \text{ , } \beta \sim Gamma(3,4)$$  for $i = 1,2,...,70,71$
\item The level of this model increased from 2 to 3.
\item This is Hierarchical Model.
\end{itemize}
\end{block}
\end{frame}

%------------------------------------------------
\section{Latent Dirichlet Allocation}
%------------------------------------------------
\begin{frame}
\sectionpage
\end{frame}
%------------------------------------------------------

\begin{frame}
\frametitle{Latent Dirichlet Allocation}
\textbf{Latent Dirichlet Allocation}
\begin{enumerate}
\item A classic example of Hierarchical Model
\item Analyze the model of Text Data
\end{enumerate}
\end{frame}
%------------------------------------------------


\subsection{Model Initialization}
\begin{frame}
\frametitle{From Beta Distribution to Dirichlet}
\begin{itemize}
\item Beta-Binomial is a conjugate distribution.
\item $$f(x\mid \alpha, \beta) = \frac{1}{\text{Beta}(\alpha,\beta)}x^{\alpha - 1}(1-x)^{\beta - 1}$$ for $x \in (0,1)$
\item Dirichlet-Multinomial is a conjugate distribution
\item $$f(x_1, x_2, ... , x_n \mid \alpha_1, ...,\alpha_n) = \frac{\Gamma (\alpha_1 + ... + \alpha_n)}{\Gamma (\alpha_1)...\Gamma (\alpha_n)}x_{1}^{\alpha_1 - 1}x_{2}^{\alpha_2 - 1}...x_{n}^{\alpha_n - 1}$$
\item $x_1,x_2,...,x_{n-1} \in (0,1) , x_1+x_2+...+x_{n-1} < 1 , x_n = 1 - (x_1 + ... + x_{n-1})$
\item $$ \text{Dir}(\vec{p} \mid \vec{\alpha}) \times \text{MultiCount}(\vec{n}) =  \text{Dir}(\vec{p} \mid \vec{\alpha} + \vec{n})$$
\end{itemize}
\end{frame}

%-------------------------------------------------------
\begin{frame}
\frametitle{Notation and Assumption}
\begin{itemize}
\item A Vocabulary indexed by $\{1,2,...,V\}$
\item A {\em word} is the basic unit of discrete data and is represented by a V-vector s.t. $$w^v = 1 \text{ and } w^u = 0 \text{ for } u \neq v$$
\item For example $$w_i = (0,0,1,0,0...,0)$$ If the ith {\em word}  matches the 3rd word in vocabulary
\item A {\em document} is a sequence of N words denoted by ${\bf w} = (w_1,w_2,...,w_N)$
\item A {\em corpus} is a collection of M documents denoted by ${\bf D} = \{ {\bf w_1},{\bf w_2},...,{\bf w_M} \}$
\item There are k topics in total.
\item {\em Bag-of-words} Assumption (Exchangeable)
\end{itemize}
\end{frame}

%------------------------------------------------------
\subsection{Where is the "Latent" in LDA?}
\begin{frame}
\frametitle{Unigram Model}
\begin{figure}
\includegraphics[width=1.1\linewidth]{fig2.png}
\end{figure}
\centering{Figure by Rickjin}\\
 (http://cos.name/2013/03/lda-math-lda-text-modeling/)
\end{frame}

%------------------------------------------------------

\begin{frame}
\frametitle{Latent Dirichlet Allocation}
{\em Latent Dirichlet Allocation} by \cite{p1}
\begin{figure}
\includegraphics[width=0.7\linewidth]{fig6.png}\\
\footnotesize{\centering{Figure by \cite{p1}}}
\end{figure}
\begin{itemize}
\item $$w \mid \beta, z \sim \text{Multinomial} $$
$$z \mid \theta \sim \text{Multinomial}(\theta)$$
$$\theta \sim \text{Dirichlet}(\alpha)$$
\item So $\alpha$ and $\beta$ are the Hyperparameters in this model. \#($k + kV$)
\end{itemize}
\end{frame}

%------------------------------------------------------
\begin{frame}
\frametitle{Latent Dirichlet Allocation}
\begin{figure}
\includegraphics[width=0.7\linewidth]{fig6.png}\\
\footnotesize{\centering{Figure by \cite{p1}}}
\end{figure}
\begin{itemize}
\item $$\beta = \{\beta_{ij}\}_{k \times V}$$
\item where $${\beta_{ij}} = p(w^{j} = 1 \mid z^i = 1)$$
\end{itemize}
\end{frame}
%------------------------------------------------------
\subsection{Posteriorl Inference}
\begin{frame}
\frametitle{Intractable Posterior}
\begin{itemize}
\item We want to find the posterior distribution of $\theta$ and $z$
$$p(\theta,{\bf z} \mid {\bf w}, \alpha, \beta) = \frac{p(\theta,{\bf z},{\bf w}\mid \alpha \beta)}{p({\bf w} \mid \alpha, \beta)}$$
\item However, the posterior distribution is intractable. (Denominator Part)
\item How to get the posterior? 
\end{itemize}
\end{frame}

%-----------------------------------------------------------
%Reference
%------------------------------------------------
\begin{frame}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99} % Beamer does not support BibTeX so references must be inserted manually as below
\bibitem[D. Blei et al., 2003]{p1} D. Blei, A. Ng, and M. Jordan. (2003)
\newblock Latent Dirichlet Allocation
\newblock \emph{Journal of Machine Learning Research} 3:993-1022.
\bibitem[K. Nigam et al., 2000]{p2} K. Nigam, A.McCallum, S. Thrun, and T. Mitchell (2000)
\newblock Text classification from labeled and unlabeled documents using EM.
\newblock \emph{Machine Learning} 39(2/3):103-134
\bibitem[A. Gelman et al., 2014]{p3} A. Gelman, J.B. Carlin, H.S. Stern, D.B. Dunson, A. Vehtari, and D.B. Rubin (2013)
\newblock Bayesian Data Analysis
\newblock \emph{CRC Press} 39(2/3):101-103
\end{thebibliography}
}
\end{frame}


%------------------------------------------------

\begin{frame}
\frametitle{The End}
\Huge{\centerline{Thank You!}}
\end{frame}

%----------------------------------------------------------------------------------------
\end{document} 